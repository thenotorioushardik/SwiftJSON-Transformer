{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all JSONL files into multiple chunks of 1000 files each\n",
    "\n",
    "def merge_jsonl_files(input_folder, output_folder, chunk_size=1000):\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith(\".jsonl\")]\n",
    "    total_files = len(files)\n",
    "    \n",
    "    # Counter for creating different chunk files\n",
    "    chunk_counter = 1\n",
    "    \n",
    "    for i in range(0, total_files, chunk_size):\n",
    "        chunk_files = files[i:i + chunk_size]\n",
    "        output_file = os.path.join(output_folder, f'merged_chunk_{chunk_counter}.jsonl')\n",
    "        \n",
    "        with open(output_file, 'w') as outfile:\n",
    "            for filename in chunk_files:\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "                with open(file_path, 'r') as infile:\n",
    "                    for line in infile:\n",
    "                        outfile.write(line)  # Append each line to the output file\n",
    "        \n",
    "        print(f\"Chunk {chunk_counter} created: {output_file}\")\n",
    "        chunk_counter += 1\n",
    "\n",
    "input_folder = '/path/to/input_folder'  # Update with the actual input folder path\n",
    "output_folder = '/path/to/output_folder'  # Update with the actual output folder path\n",
    "\n",
    "# Usage\n",
    "merge_jsonl_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of lines in JSONL files\n",
    "\n",
    "def count_lines_in_jsonl_files(directory_path):\n",
    "    total_lines = 0\n",
    "    \n",
    "    # Loop through all the files in the specified directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.jsonl'):  # Check if it's a JSONL file\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Open the JSONL file and count the lines\n",
    "            with open(file_path, 'r') as file:\n",
    "                total_lines += sum(1 for line in file)\n",
    "    \n",
    "    return total_lines\n",
    "\n",
    "# Specify the directory containing your JSONL files\n",
    "directory_path = '/path/to/jsonl_directory'  # Update with the actual directory path\n",
    "result = count_lines_in_jsonl_files(directory_path)\n",
    "print(f\"Total number of lines in all JSONL files: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSONL files into CSV files\n",
    "\n",
    "def same_length(flattened: dict):\n",
    "    max_len = max((len(v) for v in flattened.values() if isinstance(v, list)), default=0)\n",
    "    for key in flattened.keys():\n",
    "        if isinstance(flattened[key], list) and len(flattened[key]) < max_len:\n",
    "            flattened[key].extend([None] * (max_len - len(flattened[key])))\n",
    "    return flattened\n",
    "\n",
    "def process_value(keys, value, flattened):\n",
    "    if isinstance(value, dict):\n",
    "        for key in value.keys():\n",
    "            process_value(keys + [key], value[key], flattened)\n",
    "    elif isinstance(value, list):\n",
    "        for v in value:\n",
    "            process_value(keys, v, flattened)\n",
    "    else:\n",
    "        jkey = '__'.join(keys)\n",
    "        if jkey in flattened:\n",
    "            if isinstance(flattened[jkey], list):\n",
    "                flattened[jkey].append(value)\n",
    "            else:\n",
    "                flattened[jkey] = [flattened[jkey], value]\n",
    "        else:\n",
    "            flattened[jkey] = value\n",
    "\n",
    "def flatten_json(json_data):\n",
    "    flattened_result = {}\n",
    "    json_list = json_data if isinstance(json_data, list) else [json_data]\n",
    "    for j in json_list:\n",
    "        for key in j.keys():\n",
    "            process_value([key], j[key], flattened_result)\n",
    "    return flattened_result\n",
    "\n",
    "# Folder containing JSONL files\n",
    "input_folder = \"/path/to/input_folder/\"  # Update with the actual input folder path\n",
    "\n",
    "# Folder to store the output files\n",
    "output_folder = \"/path/to/output_folder/\"  # Update with the actual output folder path\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Get all .jsonl files in the folder\n",
    "input_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.jsonl')]\n",
    "\n",
    "for input_file in input_files:\n",
    "    flattened_list = []\n",
    "\n",
    "    try:\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    json_obj = json.loads(line.strip())  # Load each JSON object\n",
    "                    flat = flatten_json(json_obj)  # Flatten using the flatten_json function\n",
    "                    flattened_list.append(same_length(flat))  # Ensure same-length columns\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON line in {input_file}\")\n",
    "\n",
    "    except (FileNotFoundError, PermissionError, OSError):\n",
    "        print(f\"Error opening file: {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Convert list of dicts to DataFrame using pandas\n",
    "    df = pd.DataFrame(flattened_list)\n",
    "\n",
    "    # Adjust partitioning as needed based on dataset size\n",
    "    ddf = dd.from_pandas(df, npartitions=50)\n",
    "\n",
    "    # Output file path with the same name as the input file but with .csv extension\n",
    "    output_file = os.path.join(output_folder, os.path.splitext(os.path.basename(input_file))[0] + \".csv\")\n",
    "\n",
    "    # Write the DataFrame to a CSV file using Dask\n",
    "    ddf.to_csv(output_file, index=False, encoding='utf-8', single_file=True)\n",
    "    print(f\"Successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all csv files\n",
    "\n",
    "files = glob.glob(\"/path/to/input_folder/*.csv\") # Update with actual path\n",
    "output_file = \"/path/to/output_folder/output.csv\" # Update with actual path\n",
    "\n",
    "# Process in chunks and append data\n",
    "with open(output_file, \"w\", encoding=\"utf-8\", newline='') as outfile:\n",
    "    first_file = True  # Flag to track if it's the first file\n",
    "    for file in files:\n",
    "        for chunk in pd.read_csv(file, dtype=str, low_memory=False, chunksize=100000):  # Process 100K rows at a time\n",
    "            chunk.to_csv(outfile, index=False, header=first_file, mode=\"a\")  # Only first file has headers\n",
    "            first_file = False  # Disable headers for the next files\n",
    "\n",
    "print(\"Merging completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of rows in the CSV files\n",
    "\n",
    "csv_files = glob.glob(\"/path/to/csv_folder/*.csv\") # Update with actual path\n",
    "\n",
    "row_count = 0\n",
    "\n",
    "for file in csv_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        reader = csv.reader(f)  # Correctly reads CSV with multi-line rows\n",
    "        header = next(reader, None)  # Skip header if present\n",
    "        row_count += sum(1 for _ in reader)  # Count rows properly\n",
    "\n",
    "print(f\"Total number of rows: {row_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
